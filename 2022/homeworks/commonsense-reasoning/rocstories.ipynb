{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8bU8lIUl52o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers  # Used to compute BERT embeddings.\n",
        "\n",
        "import IPython.display\n",
        "IPython.display.clear_output()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fYZAsBxxwaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import csv\n",
        "import random\n",
        "import IPython.display\n",
        "\n",
        "# from tqdm.autonotebook import tqdm\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLMIU25mH574",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Understanding\n",
        "When you read a story, you understand many things about it.\n",
        "You understand who the characters are, where the story is taking place, and the events that have happened so far. Given your comprehension of the story so far and your common-sense understanding of the world, you can often predict what will or will not happen next.\n",
        "\n",
        "This skill is innate for you, but the ability to guess at likely future directions for a story is actually a really difficult task to teach computers. In this homework, you will be exploring two tasks that were designed to evaluate how well computers can tell a probable story continuation from an improbable one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAAJmHWfdOkk",
        "colab_type": "text"
      },
      "source": [
        "# ROCStories\n",
        "The [ROCStories task](https://cs.rochester.edu/nlp/rocstories/) involves predicting which sentence best ends a short story. The stories look something like this:\n",
        "\n",
        "**Story**\n",
        "```\n",
        "Dorothy's cat was pregnant.\n",
        "She didn't know how it happened.\n",
        "She convinced the family to keep the kittens.\n",
        "It wound up having 7 kittens.\n",
        "```\n",
        "**Candidate Ending 1**\n",
        "```\n",
        "Dorothy made sure to buy lots of cat food.\n",
        "```\n",
        "**Candidate Ending 2**\n",
        "```\n",
        "Dorothy went to the pet store and bought a new hamster.\n",
        "```\n",
        "\n",
        "The bad ending sentences are designed to be on topic but clearly incorrect to a human. Despite Ending 2 mentioning a pet store, you should have quickly guessed that Ending 1 is the correct one.\n",
        "\n",
        "The tricky part about ROCStories is that the training set only contains 5-sentence stories with good ending sentences.\n",
        "However, at test time you see two possible 5th sentences and need to classify which is better.\n",
        "You can read up on the dataset and how it was collected in the [paper introducing the dataset](https://www.aclweb.org/anthology/N16-1098.pdf).\n",
        "\n",
        "In those homework, you will investigate how a very simple sentiment-based approach can get reasonable accuracy at this task, revealing the challenges behind designing datasets and tasks for evaluating natural language and commonsense understanding.\n",
        "\n",
        "You will also train a neural network to perform the task, hopefully achieving higher accuracy.\n",
        "\n",
        "Lastly, if you choose to, you can submit to the official leaderboard for extra credit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCX9rCKd0ucN",
        "colab_type": "text"
      },
      "source": [
        "### Download the data\n",
        "There are two versions of the ROCStories dataset. After its original 2016 release, researchers noticed problematic biases in the data. The 2017/2018 version was an attempt to resolve these biases. You should report your results on both the 2016 and 2018 validation sets, as well as the 2016 test set. (The 2018 test set is available for download online as well, but the labels for it are still being withheld.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOpKek9JofUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Download the data\n",
        "!mkdir rocstories_data\n",
        "!wget -nc -O rocstories_data/train2017.csv https://docs.google.com/spreadsheets/d/1emH8KL8NVCCumZc2oMu-3YqRWddD3AqZEHvNqMdfgKA/export?format=csv\n",
        "!wget -nc -O rocstories_data/valid2018.csv https://docs.google.com/spreadsheets/d/1F9vtluzD3kZOn7ULKyMQZfoRnSRzRnnaePyswkRqIdY/export?format=csv\n",
        "!wget -nc -O rocstories_data/valid2016.csv https://docs.google.com/spreadsheets/d/1FkdPMd7ZEw_Z38AsFSTzgXeiJoLdLyXY_0B_0JIJIbw/export?format=csv\n",
        "!wget -nc -O rocstories_data/test2016.csv  https://docs.google.com/spreadsheets/d/11tfmMQeifqP-Elh74gi2NELp0rx9JMMjnQ_oyGKqCEg/export?format=csv\n",
        "\n",
        "IPython.display.clear_output()  # Clear the stdout/\n",
        "\n",
        "def read_rocstories_valid_csv(path):\n",
        "  examples = []\n",
        "  with open(path) as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for line in reader:\n",
        "      context = [line['InputSentence1'], line['InputSentence2'],\n",
        "                 line['InputSentence3'], line['InputSentence4']]\n",
        "      option_0 = line['RandomFifthSentenceQuiz1']\n",
        "      option_1 = line['RandomFifthSentenceQuiz2']\n",
        "      label = int(line['AnswerRightEnding']) - 1\n",
        "      examples.append({'context': context, \n",
        "                       'options': [option_0, option_1],\n",
        "                       'label': label})\n",
        "  return examples\n",
        "\n",
        "def read_rocstories_train_csv(path):\n",
        "  examples = []\n",
        "  with open(path) as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for line in reader:\n",
        "      story = [line['sentence1'], line['sentence2'],\n",
        "               line['sentence3'], line['sentence4'],\n",
        "               line['sentence5']]\n",
        "      examples.append({'story': story})\n",
        "  return examples\n",
        "\n",
        "train_data = read_rocstories_train_csv('/content/rocstories_data/train2017.csv')\n",
        "valid_2016_data = read_rocstories_valid_csv('/content/rocstories_data/valid2016.csv')\n",
        "valid_2018_data = read_rocstories_valid_csv('/content/rocstories_data/valid2018.csv')\n",
        "test_2016_data = read_rocstories_valid_csv('/content/rocstories_data/test2016.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM9JRPj-t-c1",
        "colab_type": "text"
      },
      "source": [
        "Here's what an example in the train dataset looks like:\n",
        "```\n",
        "> print(train_data[123])\n",
        "{'story': [\"Sam's dog Rex escaped from their yard.\",\n",
        "           'Sam was distraught.',\n",
        "           'He went out calling for Rex.',\n",
        "           'Then he saw Rex come running up the street!',\n",
        "           'Sam was so relieved, he almost cried!']}\n",
        "```\n",
        "Here's what an example in one of the validation datasets looks like:\n",
        "```\n",
        "> print(valid_2016_data[123])\n",
        "{'context': [\"Jen got sent to her aunt's for the summer.\",\n",
        "            'She hated the thought of being away from her local library all summer.',\n",
        "            'She took a few books with her but she would go through those quickly.',\n",
        "            'When she arrived her aunt took her into a special room in her home.'],\n",
        " 'label': 1,\n",
        " 'options': ['Jen saw her books burning in the fireplace.',\n",
        "             'The room was full of shelves of books that appealed to girls.']}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCLvj-CfG7h",
        "colab_type": "text"
      },
      "source": [
        "### Classify with sentiment analysis\n",
        "After the ROCStories dataset was released in 2016, researchers soon realized that it has undesired biases. The correct next sentences tends to be more positive than the incorrect next sentences. An updated version of the dataset was released in 2018 that attempted to eliminate this bias.\n",
        "\n",
        "Implement a function that makes a prediction based on sentiment.\n",
        "You can use either [AllenNLP](https://demo.allennlp.org/sentiment-analysis) or [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis).\n",
        "Your function should compute the sentiment of each 5th sentence option and predict the one with the more positive sentiment.\n",
        "\n",
        "**In your report, list the validation and test accuracies you get with your sentiment classifier. Also show a couple of negative examples where the model incorrectly,**\n",
        "\n",
        "*Hint: We were able to get over 60\\% accuracy using only the sentiment of the 5th sentences, but you can also experiment with running sentiment analysis on the context sentences as well to see if you can improve upon this.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIGF4DyixSst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes an accuracy given the data dictionary and a list of [0, 1] predictions.\n",
        "\n",
        "def compute_accuracy(data, predictions):\n",
        "  ground_truth = np.array([ex['label'] for ex in data])\n",
        "  predictions = np.array(predictions)\n",
        "  assert len(ground_truth) == len(predictions)\n",
        "\n",
        "  return np.sum(np.equal(ground_truth, predictions)) / float(len(ground_truth))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO2Ohaw3s3dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_based_on_sentiment(data):\n",
        "  \"\"\"Returns a list with one value per example in data.\n",
        "\n",
        "  List values should either be 0 or 1 indicating which ending is predicted.\n",
        "  \"\"\"\n",
        "  #### TODO: YOUR IMPLEMENTATION HERE ####\n",
        "  predictions = [0] * len(data)\n",
        "  return predictions\n",
        "\n",
        "predictions_valid_2016 = predict_based_on_sentiment(valid_2016_data)\n",
        "print('\\n2016 validation accuracy: ' )\n",
        "print(compute_accuracy(valid_2016_data, predictions_valid_2016))\n",
        "\n",
        "predictions_valid_2018 = predict_based_on_sentiment(valid_2018_data)\n",
        "print('\\n2018 validation accuracy: ' )\n",
        "print(compute_accuracy(valid_2018_data, predictions_valid_2018))\n",
        "\n",
        "predictions_test_2016 = predict_based_on_sentiment(test_2016_data)\n",
        "print('\\n2016 test accuracy: ' )\n",
        "print(compute_accuracy(test_2016_data, predictions_test_2016))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsjm0sJ12Doi",
        "colab_type": "text"
      },
      "source": [
        "### Train a classifier using BERT embeddings.\n",
        "**Important: Go to `Runtime > Change runtime type` and make sure you have a GPU in your runtime before completing this section.**\n",
        "\n",
        "In this section, you'll train a classifier to predict which ending is correct. Ideally, you'd finetune a large pre-trained language model (a.k.a. BERT) on the classification task, but since finetuning is pretty slow, we'll instead pre-compute BERT embeddings for each story context and for each possible ending. We'll then train a new model on top of these pre-computed embeddings.\n",
        "\n",
        "**But how do we do classification if the training set only contains positive examples?**\n",
        "\n",
        "We invent negative examples! At each training step, we pick a random set of 5th sentences from all of the 5th sentences in the training set to act as distractors.\n",
        "The hyperparameter `NUM_CANDIDATES` sets the number of distractors that are chosen. If `NUM_CANDIDATES` is set to 50, that means we do 50-way classification in our loss.\n",
        "\n",
        "**What should the neural network look like?**\n",
        "\n",
        "The goal of the neural network is to project the embedding of the context into the embedding space of endings. \n",
        "This way at evaluation time, we can compute a score for each candidate ending by taking the dot-product between the predicted embedding returned by the neural network and the embeddings of each ending. Whichever ending has the highest score wins.\n",
        "\n",
        "You are free to implement the neural network however you'd like, but you'll probably want to start with a simple [MLP](https://www.tensorflow.org/guide/keras/overview#sequential_model). (You'll want to omit the final softmax layer since that's taken care of for you in the train loop.)\n",
        "\n",
        "\n",
        "**What you should complete in this section:**\n",
        "* **Fill in the `get_model` function. Try at least two different architectures (varying the number of layers, hidden size, activation functions, etc.) and include a discussion of their relative performance in your report.**\n",
        "* **Try training with at least two different hyperparameter settings to see if you can improve performance. Include a discussion of the experiments you tried and their performance in your report.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo9HwvWGnUGz",
        "colab_type": "text"
      },
      "source": [
        "#### Compute/retrieve BERT embeddings.\n",
        "\n",
        "Note that we've commented out the lines to generate the BERT embeddings and instead provided you with pre-computed files since running BERT on 100k+ sequences can take a few hours.\n",
        "\n",
        "However, you are welcome to uncomment and experiment with computing the embeddings yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsuhEsU-L9jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "def load_bert():\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  model = BertModel.from_pretrained('bert-base-uncased')\n",
        "  return model, tokenizer\n",
        "\n",
        "def bert_embedding(text):\n",
        "  inputs_ids = TOKENIZER.encode(text)\n",
        "  input_ids = torch.tensor(inputs_ids).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "  _, merged_embedding = BERT_MODEL(input_ids)\n",
        "  return merged_embedding.detach().numpy()\n",
        "  \n",
        "def get_train_embeddings(data):\n",
        "  \"\"\"Computes embeddings for each example in the provided train set.\"\"\"\n",
        "  context_embeddings = []\n",
        "  ending_embeddings = []\n",
        "  print('Starting')\n",
        "  # for example in tqdm(data, desc='Computing BERT embeddings '):\n",
        "  for idx, example in enumerate(data):\n",
        "    if idx % 20 == 0:\n",
        "      print('{}/{}'.format(idx+1, len(data)))\n",
        "      print(' '.join(example['story']))\n",
        "    context_embedding = bert_embedding(' '.join(example['story'][:4]))\n",
        "    ending_embedding = bert_embedding(example['story'][4])\n",
        "\n",
        "    context_embeddings.append(context_embedding)\n",
        "    ending_embeddings.append(ending_embedding)\n",
        "  context_embeddings = np.concatenate(context_embeddings, axis=0)\n",
        "  ending_embeddings = np.concatenate(ending_embeddings, axis=0)\n",
        "  return context_embeddings, ending_embeddings\n",
        "\n",
        "def get_valid_embeddings(data):\n",
        "  \"\"\"Computes embeddings for each example in the provided validation set.\"\"\"\n",
        "  context_embeddings = []\n",
        "  ending_0_embeddings = []\n",
        "  ending_1_embeddings = []\n",
        "  for example in tqdm(data, desc='Computing BERT embeddings '):\n",
        "    context_embedding = bert_embedding(' '.join(example['context'][:4]))\n",
        "    ending_0_embedding = bert_embedding(example['options'][0])\n",
        "    ending_1_embedding = bert_embedding(example['options'][1])\n",
        "\n",
        "    context_embeddings.append(context_embedding)\n",
        "    ending_0_embeddings.append(ending_0_embedding)\n",
        "    ending_1_embeddings.append(ending_1_embedding)\n",
        "\n",
        "  context_embeddings = np.concatenate(context_embeddings, axis=0)\n",
        "  ending_0_embeddings = np.concatenate(ending_0_embeddings, axis=0)\n",
        "  ending_1_embeddings = np.concatenate(ending_1_embeddings, axis=0)\n",
        "  return context_embeddings, ending_0_embeddings, ending_1_embeddings\n",
        "\n",
        "# These are the lines I used to generate BERT embeddings. Since, they are slow\n",
        "# to compute, we've provided the outputs as .pkl files.\n",
        "# BERT_MODEL, TOKENIZER = load_bert()\n",
        "# train_context_embs, train_ending_embs = get_train_embeddings(train_data)\n",
        "# valid_2016_context_embs, valid_2016_ending_0_embs, valid_2016_ending_1_embs = get_valid_embeddings(valid_2016_data)\n",
        "# valid_2018_context_embs, valid_2018_ending_0_embs, valid_2018_ending_1_embs = get_valid_embeddings(valid_2018_data)\n",
        "# test_2016_context_embs, test_2016_ending_0_embs, test_2016_ending_1_embs = get_valid_embeddings(test_2018_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDP_U43qlFzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://cis700_shared_data/rocstories_data/rocstories_train.pkl /content/rocstories_train.pkl\n",
        "with open('/content/rocstories_train.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "  train_context_embs = data['contexts']\n",
        "  train_ending_embs = data['endings']\n",
        "\n",
        "!gsutil cp gs://cis700_shared_data/rocstories_data/rocstories_valid_2016.pkl /content/rocstories_valid_2016.pkl\n",
        "with open('/content/rocstories_valid_2016.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "  valid_2016_context_embs = data['contexts']\n",
        "  valid_2016_ending_0_embs = data['endings_0']\n",
        "  valid_2016_ending_1_embs = data['endings_1']\n",
        "\n",
        "!gsutil cp gs://cis700_shared_data/rocstories_data/rocstories_valid_2018.pkl /content/rocstories_valid_2018.pkl\n",
        "with open('/content/rocstories_valid_2018.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "  valid_2018_context_embs = data['contexts']\n",
        "  valid_2018_ending_0_embs = data['endings_0']\n",
        "  valid_2018_ending_1_embs = data['endings_1']\n",
        "\n",
        "!gsutil cp gs://cis700_shared_data/rocstories_data/rocstories_test_2016.pkl /content/rocstories_test_2016.pkl\n",
        "with open('/content/rocstories_test_2016.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "  test_2016_context_embs = data['contexts']\n",
        "  test_2016_ending_0_embs = data['endings_0']\n",
        "  test_2016_ending_1_embs = data['endings_1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWBaQAX-rosb",
        "colab_type": "text"
      },
      "source": [
        "#### Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Cile0a0xrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(batch_size, num_candidates):\n",
        "  \"\"\"Returns a single training batch.\n",
        "  \n",
        "  Returns:\n",
        "  batch_inputs: [batch_size, embedding_size] matrix of context embeddings.\n",
        "  batch_candidates: [num_candidates, embedding_size] matrix of embeddings of \n",
        "    candidate 5th sentence embeddings. The groundtruth 5th sentence for the ith\n",
        "    example in batch_inputs is in the ith row of batch_candidates.\n",
        "  labels: [batch_size] For each example in batch_inputs, the index of the true\n",
        "    5th sentence in batch_candidates.\n",
        "  \"\"\"\n",
        "  if num_candidates < batch_size:\n",
        "    raise ValueError(\n",
        "        'At minimum the number of candidates is at least all of the other 5th '\n",
        "        'sentences in the batch.')\n",
        "    \n",
        "  batch_inputs = []\n",
        "  batch_candidates = []\n",
        "  batch_labels = []\n",
        "  for i in range(batch_size):\n",
        "    rand_ex_index = random.randint(0, train_context_embs.shape[0]-1)\n",
        "    batch_inputs.append(train_context_embs[rand_ex_index, :])\n",
        "    batch_candidates.append(train_ending_embs[rand_ex_index, :])\n",
        "    # The true next embedding is in the ith position in the candidates\n",
        "    batch_labels.append(i)\n",
        "\n",
        "  # Increase the number of \"distractor\" candidates to num_candidates.\n",
        "  for i in range(num_candidates - batch_size):\n",
        "    rand_ex_index = random.randint(0, train_context_embs.shape[0]-1)\n",
        "    batch_candidates.append(train_ending_embs[rand_ex_index, :])\n",
        "\n",
        "  batch_inputs = np.stack(batch_inputs, axis=0)\n",
        "  batch_candidates = np.stack(batch_candidates, axis=0)\n",
        "  return batch_inputs, batch_candidates, batch_labels\n",
        "\n",
        "def predict_based_on_bert_classifier(\n",
        "    context_embs, ending_0_embs, ending_1_embs, model):\n",
        "  \"\"\"Returns a list of predictions based on model.\"\"\"\n",
        "  predicted_embs = model(context_embs)\n",
        "  \n",
        "  predictions = []\n",
        "  for idx in range(predicted_embs.shape[0]):\n",
        "    pred_emb = predicted_embs[idx, :]\n",
        "    score_0 = np.dot(pred_emb, ending_0_embs[idx, :])\n",
        "    score_1 = np.dot(pred_emb, ending_1_embs[idx, :])\n",
        "    predictions.append(score_0 < score_1)\n",
        "  return predictions\n",
        "  \n",
        "def get_model():\n",
        "  \"\"\"Returns a Keras model.\n",
        "  The model should input a [batch_size, embedding_size] tensor and output a new\n",
        "  [batch_size, embedding_size] tensor. At it's simplest, it could just be a\n",
        "  single dense layer. You should experiment with adding layers, changing the\n",
        "  activation function, or otherwise modifying the architecture defined below.\n",
        "  See:\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  # This is an example of a very simple network consisting of a single nonlinear\n",
        "  # layer followed by a linear projection back to the BERT embedding size.\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(512, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(768, activation=\"linear\"))\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-v8K2mDiuFO",
        "colab_type": "text"
      },
      "source": [
        "You should experiment with the hyperparamters (IN ALL_CAPS) below to see if you can improve performance. I was able to get 67% validation accuracy with the provided values and using a two-layer network with a ReLU nonlinearity between the layers. Training took about an hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i38yZvqkZIj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### HYPERPARAMETERS ####\n",
        "NUM_TRAIN_STEPS = 10000  # How many step to train for.\n",
        "BATCH_SIZE = 32  # Number of examples used in step of training.\n",
        "NUM_CANDIDATES = 50  # Number of candidate 5th sentences classifier must decide between.\n",
        "LEARNING_RATE = 0.001  # Learning rate.\n",
        "# If your loss is barely going down, learning rate might be too small.\n",
        "# If your loss is jumping around, it might be too big.\n",
        "\n",
        "# You may experiment with other optimizers or loss functions if you'd like.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model = get_model()\n",
        "\n",
        "# Iterate over the batches of a dataset.\n",
        "for train_step in range(NUM_TRAIN_STEPS):\n",
        "  with tf.GradientTape() as tape:\n",
        "    batch_inputs, batch_candidates, batch_labels = get_batch(BATCH_SIZE, NUM_CANDIDATES)\n",
        "\n",
        "    # Predicted 5th sentence embedding for each batch position/\n",
        "    outputs = model(batch_inputs)\n",
        "    # The logits will be batch_size * num_candidates, giving a score for each\n",
        "    # candidate 5th sentence. We'd like the true 5th sentence to have the\n",
        "    # highest score.\n",
        "    logits = tf.matmul(outputs, batch_candidates, transpose_b=True)\n",
        "    # Loss value for this minibatch\n",
        "    loss_value = loss_fn(batch_labels, logits)\n",
        "\n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "  if train_step % 100 == 0:\n",
        "    print('Step {}, batch_train_loss={}'.format(train_step, loss_value))\n",
        "  if train_step % 1000 == 0:\n",
        "    predictions_2016 = predict_based_on_bert_classifier(valid_2016_context_embs, valid_2016_ending_0_embs, valid_2016_ending_1_embs,model)\n",
        "    predictions_2018 = predict_based_on_bert_classifier(valid_2018_context_embs, valid_2018_ending_0_embs, valid_2018_ending_1_embs,model)\n",
        "    \n",
        "    print('2016 validation accuracy: {}'.format(compute_accuracy(valid_2016_data, predictions_2016)))\n",
        "    print('2018 validation accuracy: {}'.format(compute_accuracy(valid_2018_data, predictions_2018)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7gyObIurdVQ",
        "colab_type": "text"
      },
      "source": [
        "**What is overfitting?**\n",
        "\n",
        "You may have observed when training that your validation accuracy goes up for a while and then eventually starts going down. This is called overfitting, because your model is learning to be really good at classifying examples from the training set at the expense of dong a good job at classifying usneen exampes in the validation set. You could prevent overfitting by automatically stopping training when the validation accuracy has not improved in X steps (where X is another hyperparamter you'd have to decide upon)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMjv10QFrdnd",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EZCCuQLRcEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_2016 = predict_based_on_bert_classifier(\n",
        "    valid_2016_context_embs, valid_2016_ending_0_embs, valid_2016_ending_1_embs,\n",
        "    model)\n",
        "print('\\n2016 validation accuracy: ' )\n",
        "print(compute_accuracy(valid_2016_data, predictions_2016))\n",
        "\n",
        "predictions_2018 = predict_based_on_bert_classifier(\n",
        "    valid_2018_context_embs, valid_2018_ending_0_embs, valid_2018_ending_1_embs,\n",
        "    model)\n",
        "print('\\n2018 validation accuracy: ' )\n",
        "print(compute_accuracy(valid_2018_data, predictions_2018))\n",
        "\n",
        "predictions_2016 = predict_based_on_bert_classifier(\n",
        "    test_2016_context_embs, test_2016_ending_0_embs, test_2016_ending_1_embs,\n",
        "    model)\n",
        "print('\\n2016 test accuracy: ' )\n",
        "print(compute_accuracy(test_2016_data, predictions_2016))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw0LAKC9O9zy",
        "colab_type": "text"
      },
      "source": [
        "### Train classifier on validation set\n",
        "Part of the difficulty (and interestingness) of the ROCStories task is that the training set contains only positive examples. However, researchers have found that accuracies as high as 90% are possible if you cheat and train a supervised classifier using the examples with both positive and negative examples found in the validation set.\n",
        "\n",
        "**Run the train code below, experimenting with at least twos variant, either modifying the hyperparamters or model architecture. You could also try to find a way to take advantage of the large unlabeled dataset in addition to the labeled data. Include a discussion in your report.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enNKm3zYnwba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch_from_valid(batch_size, inputs, labels):\n",
        "  \"\"\"Returns a single training batch extracted form the validation set.\n",
        "\n",
        "  Inputs:\n",
        "  batch_size: The batch size.\n",
        "  inputs: [dataset_size, 2*embedding_size] matrix of all inputs in the training\n",
        "    set.\n",
        "  labels: [dataset_size] for each example, 0 if example has the incorrect ending\n",
        "    embedding, 1 if it has the correct ending embedding.\n",
        "  \n",
        "  Returns:\n",
        "  batch_inputs: [batch_size, 2*embedding_size] matrix of embeddings (each\n",
        "    embedding is a context embedding concatenated with an ending embedding).\n",
        "  labels: [batch_size] For each example in batch_inputs, contains either 0 or 1,\n",
        "    indicating whether the 5th ending is the correct one.\n",
        "  \"\"\"\n",
        "  batch_inputs = []\n",
        "  batch_labels = []\n",
        "  for i in range(batch_size):\n",
        "    rand_ex_index = random.randint(0, inputs.shape[0]-1)    \n",
        "    batch_inputs.append(inputs[rand_ex_index, :])\n",
        "    batch_labels.append(labels[rand_ex_index])\n",
        "    \n",
        "  batch_inputs = np.stack(batch_inputs, axis=0)\n",
        "  return batch_inputs, batch_labels\n",
        "\n",
        "# Each input example consists of a context_embedding concatenated with an ending embedding.\n",
        "def build_dataset():\n",
        "  \"\"\"Builds a dataset out of the validation set examples.\n",
        "\n",
        "  Each example in valid_2016 and valid_2018 becomes two exampes in this new \n",
        "  dataset:\n",
        "  * one where ending_0's embedding is concatenated to the context embedding\n",
        "  * one where ending_1's embedding is concatenated to the context embedding\n",
        "\n",
        "  The label for each example is 1 if the correct ending's embedding is present,\n",
        "  0 if the incorrect ending's embedding is present.\n",
        "\n",
        "  Returns:\n",
        "  all_inputs: [new_dataset_size, embedding_size*2]\n",
        "  all_labels: [new_dataset_size]\n",
        "  \"\"\"\n",
        "  inputs_2016 = tf.concat(\n",
        "      [tf.concat([valid_2016_context_embs, valid_2016_ending_0_embs], axis=-1),\n",
        "      tf.concat([valid_2016_context_embs, valid_2016_ending_1_embs], axis=-1)], axis=0)\n",
        "  labels = [ex['label'] for ex in valid_2016_data]\n",
        "  labels_2016 = labels + [1 - label for label in labels]\n",
        "\n",
        "  inputs_2018 = tf.concat(\n",
        "      [tf.concat([valid_2018_context_embs, valid_2018_ending_0_embs], axis=-1),\n",
        "      tf.concat([valid_2018_context_embs, valid_2018_ending_1_embs], axis=-1)], axis=0)\n",
        "  labels = [ex['label'] for ex in valid_2018_data]\n",
        "  labels_2018 = labels + [1 - label for label in labels]\n",
        "\n",
        "  all_inputs = tf.concat([inputs_2016, inputs_2018], axis=0)\n",
        "  all_labels = labels_2016 + labels_2018\n",
        "\n",
        "  return all_inputs, all_labels\n",
        "\n",
        "def predict_based_on_bert_binary_classifier(\n",
        "    context_embs, ending_0_embs, ending_1_embs, model):\n",
        "  \"\"\"Returns a list of predictions based on binary classification model.\"\"\"\n",
        "  scores_ending_0 = model(tf.concat([context_embs, ending_0_embs], -1))\n",
        "  scores_ending_1 = model(tf.concat([context_embs, ending_1_embs], -1))\n",
        "  predictions = tf.greater(scores_ending_0, scores_ending_1)[:, 1]\n",
        "  return predictions\n",
        "\n",
        "def get_binary_classifier():\n",
        "  \"\"\"Returns a Keras model.\n",
        "  The model should input a [batch_size, 2*embedding_size] tensor and output a\n",
        "  [batch_size, 2] tensor. The final final dimension needs to be 2 because we are\n",
        "  doing binary classification.\n",
        "  \n",
        "  You should experiment with modifying the architecture below.\n",
        "  See:\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(512, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(2, activation=\"linear\"))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gcXfLYUL5Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_TRAIN_STEPS = 20000  # How many step to train for.\n",
        "BATCH_SIZE = 32  # Number of examples used in step of training.\n",
        "LEARNING_RATE = 0.001  # Learning rate.\n",
        "\n",
        "NUM_TRAIN_EXAMPLES = 5000 # How many examples from the valid set to use for training.\n",
        "# The remainder will be placed into a new valid set.\n",
        "\n",
        "# You should with varying NUM_TRAIN_EXAMPLES. If it is larger, you will train a \n",
        "# better model, but you will have fewer examples available your validation set\n",
        "# for tuning other hyperparameters.\n",
        "all_inputs, all_labels = build_dataset()\n",
        "train_inputs = all_inputs[:NUM_TRAIN_EXAMPLES, :]\n",
        "train_labels = all_labels[:NUM_TRAIN_EXAMPLES]\n",
        "valid_inputs = all_inputs[NUM_TRAIN_EXAMPLES:, :]\n",
        "valid_labels = all_labels[NUM_TRAIN_EXAMPLES:]\n",
        "\n",
        "# You may experiment with other optimizers or loss functions if you'd like.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model_2 = get_binary_classifier()\n",
        "\n",
        "# Iterate over the batches of a dataset.\n",
        "for train_step in range(NUM_TRAIN_STEPS):\n",
        "  with tf.GradientTape() as tape:\n",
        "    batch_inputs, batch_labels = get_batch_from_valid(\n",
        "        BATCH_SIZE, train_inputs, train_labels)\n",
        "\n",
        "    logits = model_2(batch_inputs)\n",
        "    loss_value = loss_fn(batch_labels, logits)\n",
        "\n",
        "  grads = tape.gradient(loss_value, model_2.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model_2.trainable_weights))\n",
        "\n",
        "  if train_step % 100 == 0:\n",
        "    batch_acc = sum(tf.equal(batch_labels, tf.argmax(logits, axis=-1)).numpy()) / BATCH_SIZE\n",
        "    print('Step {0}, batch_loss={1:.5f}, batch_acc={2:.3f}'.format(\n",
        "        train_step, loss_value, batch_acc))\n",
        "  if train_step % 1000 == 0:\n",
        "    valid_logits = model_2(valid_inputs)\n",
        "    num_correct = sum(tf.equal(valid_labels, tf.argmax(valid_logits, axis=-1)).numpy())\n",
        "    print('Validation accuracy: {0:.3f}'.format(num_correct / len(valid_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7onI_ScUVssT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can no longer fairly evaluate on the 2016 and 2018 validation sets since\n",
        "# they've been used for training. Instead, we only evaluate on the 2016 test set.\n",
        "\n",
        "predictions_2016 = predict_based_on_bert_binary_classifier(\n",
        "    test_2016_context_embs, test_2016_ending_0_embs, test_2016_ending_1_embs,\n",
        "    model_2)\n",
        "print('\\n2016 test accuracy: ' )\n",
        "print(compute_accuracy(test_2016_data, predictions_2016))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4UrGlvgqrT3",
        "colab_type": "text"
      },
      "source": [
        "# Extra Credit\n",
        "For extra credit, make an account on [Codalab](https://competitions.codalab.org/) and submit your best model's outputs to the [Winter 2018 leaderboard](https://competitions.codalab.org/competitions/15333#participate-submit_results). You'll need to download the Winter 2018 CSV and create BERT embeddings for it.\n",
        "\n",
        "**If you choose to do the extra credit, please take a screenshot of the Codalab leaderboard  (including your submission) and paste it into your report. Your report should also include a description of the method that you used in your submission.**"
      ]
    }
  ]
}