{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/schemas/HW5_Schemas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6gRCRlEyq2s"
      },
      "source": [
        "# HW 5: Schemas\n",
        "\n",
        "A **schema** is a structured reprensentation to hold facts or a plan, which can be tracked over time.\n",
        "\n",
        "In this homework, you will create your own schema to represent the state of a story world over time.\n",
        "\n",
        "**The purpose of this homework is to test your understanding of schemas and get hands-on experience with a state-of-the-art tool in commonsense reasoning.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZoWcXimWC4"
      },
      "source": [
        "## Your Task\n",
        "You will be creating a schema using ATOMIC to track the state of a fictional world. For each sentence of the story, you will parse it (provided), call COMET (provided, but what you input is up to you), create preconditions to determine if a sentence can be added (TODO), and create effects to use to update your schema (TODO).\n",
        "\n",
        "Let's teach your agent some basic information about the world!\n",
        "\n",
        "------------------------\n",
        "\n",
        "Formally, the task is:\n",
        "\n",
        "Given an input sentence at time *t* (*In_t*), produce a schema *S_t*. Do this for each sentence in the story.\n",
        "\n",
        "For example, using VerbNet:\n",
        "\n",
        "| Timestep | Input | Schema |\n",
        "|--------|-------|--------|\n",
        "| 1 | Bethany picks up the sword. | `Bethany: has_possesion(sword)` |\n",
        "| 2 | Bethany throws the sword. | `Bethany: !has_possesion(sword)` |\n",
        "\n",
        "*In_1* - \"Bethany picks up the sword.\"\n",
        "\n",
        "would produce\n",
        "\n",
        "*S_1* - `Bethany: has_possesion(sword)`\n",
        "\n",
        "But then if the next sentence is:\n",
        "\n",
        "*In_2* - \"Bethany throws the sword.\"\n",
        "\n",
        "The state would be updated to\n",
        "\n",
        "*S_2* - `Bethany: !has_possesion(sword)`\n",
        "\n",
        "-----------------------------\n",
        "Your resulting system will be sort of like this simplified diagram (the parser is provided for you):\n",
        "![Given a sentence_t and the knowledge representation from your knowledge database of choice, produce schemas (via some schema processor you create)](https://interactive-fiction-class.org/homeworks/schemas/schemas.png)\n",
        "\n",
        "There is some knowledge about the story world, and you are using a schema to feed this information into bite-sized chunks so that your agent (and you) can understand it.\n",
        "You will then have a processing step on the schema where you update it as you get more information as the story progresses.\n",
        "\n",
        "\n",
        "To reiterate, you will:\n",
        "1. Get to know [COMET-ATOMIC-2020](https://aaai.org/ojs/index.php/AAAI/article/view/4160). (Alternate link in case AAAI is down: https://arxiv.org/pdf/2010.05953.pdf)\n",
        "2. Make a __schema manipulator__ (not a real term, but I think it sounds cool), which will take in knowledge from ATOMIC and spit out your schema. Skeleton code is provided for you, but you are welcome to change things so that it makes more sense to you. This involves two steps:\n",
        "\n",
        "  a. Validate: Generate preconditions to determine if an event can be added.\n",
        "\n",
        "  b. Update: Generate effects to update your world state.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuI9RGzJ94X9"
      },
      "source": [
        "# What is ATOMIC?\n",
        "\n",
        "ATOMIC is a commonsense knowledge graph with some social inferences, among other things.\n",
        "\n",
        "```\n",
        "@inproceedings{sap2019atomic,\n",
        "   title={ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning},\n",
        "   author={Sap, Maarten and LeBras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A and Choi, Yejin},\n",
        "   year={2019},\n",
        "   booktitle={AAAI},\n",
        "   url={https://aaai.org/ojs/index.php/AAAI/article/view/4160}\n",
        "}\n",
        "```\n",
        "\n",
        "It contains the following inferences about people and events:\n",
        "\n",
        "* Because PersonX wanted (xIntent)\n",
        "* Before, PersonX needed (xNeed, HasPrerequisite)\n",
        "* PersonX is seen as (xAttr)\n",
        "* As a result, PersonX feels (xReact)\n",
        "* As a result, PersonX wants (xWant)\n",
        "* As a result, PersonX reasons (xReason)\n",
        "* PersonX then (xEffect)\n",
        "* As a result, others feel (oReact)\n",
        "* As a result, others want (oWant)\n",
        "* Others then (oEffect)\n",
        "* Happens before (isBefore)\n",
        "* Happens after (isAfter)\n",
        "* Is hindered by (HinderedBy)\n",
        "* Causes (Causes)\n",
        "\n",
        "and inferences about entities:\n",
        "* Is located at (AtLocation, LocatedNear, LocationOfAction)\n",
        "* Is made up of (MadeUpOf, PartOf. NotMadeOf)\n",
        "* Is used to (UsedFor, ObjectUse)\n",
        "* Has the property (HasProperty, NotHasProperty)\n",
        "* Is capable of (CapableOf, NotCapableOf)\n",
        "* Desires (Desires, NotDesires)\n",
        "\n",
        "\n",
        "Among other things (full list is in code block 41).\n",
        "Or you can play with it online here: https://mosaickg.apps.allenai.org/kg_atomic2020\n",
        "https://mosaickg.apps.allenai.org/model_comet2020_people_events\n",
        "https://mosaickg.apps.allenai.org/model_comet2020_entities\n",
        "\n",
        "\n",
        "[COMET](https://aclanthology.org/P19-1470) is the model that is trained on ATOMIC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC8ocsUWWill"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1LjnvrV-LbA"
      },
      "source": [
        "## Get COMET and Install packages\n",
        "\n",
        "Repo: https://github.com/allenai/comet-atomic-2020/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPPEQeYigKP8"
      },
      "outputs": [],
      "source": [
        "# Clone the repo\n",
        "%%capture\n",
        "!git clone https://github.com/allenai/comet-atomic-2020.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Modr9o4NWRsO"
      },
      "outputs": [],
      "source": [
        "# Enter the directory\n",
        "import os\n",
        "os.chdir('comet-atomic-2020')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I9-JvoaOTwvB"
      },
      "outputs": [],
      "source": [
        "# Install ATOMIC's dependencies\n",
        "%%capture\n",
        "!pip install rouge_score\n",
        "!pip install transformers\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukxhQxiooue2"
      },
      "source": [
        "### Also, Stanford's Stanza Parser\n",
        "\n",
        "In order to process the input sentences, you will need to do some parsing. Here, we have setup Stanford's English language NER (Named Entity Recognition) and constituency parser using [Stanza](https://stanfordnlp.github.io/stanza/index.html). You're also welcome to use any of the other parsers they provide (or even switch to a completely different type of parser that you like better)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x7KgiGoPKnQt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m pip install stanza\n",
        "\n",
        "import stanza\n",
        "stanza.download('en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaIyiwTv9kFA"
      },
      "source": [
        "**You might need to restart the runtime now.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j_IpCvvOlYjH"
      },
      "outputs": [],
      "source": [
        "# Example code to run Stanza\n",
        "\n",
        "import stanza #here's the re-import for when your runtime is restarted\n",
        "import json\n",
        "nlp = stanza.Pipeline('en', processors='tokenize, ner, mwt') #lemma, depparse, constituency, pos\n",
        "parse = nlp(\"Aurora submitted her resignation to Facebook.\")\n",
        "y = json.loads(str(parse))\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9LWpLE1VZ_F"
      },
      "source": [
        "## COMET-ATOMIC-2020 (BART) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0PMpNE3zch4z"
      },
      "outputs": [],
      "source": [
        "# Download the model\n",
        "%%capture\n",
        "!bash models/comet_atomic2020_bart/download_model.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHxOhhBYelSf"
      },
      "source": [
        "**Tip: Take note of the `all_relations` dictionary below! You will need it later on.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OQmrCl-pcSKf"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "# copied from models/comet_atomic2020_bart/generation_example.py\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from models.comet_atomic2020_bart.utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
        "\n",
        "\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "\n",
        "class Comet:\n",
        "    def __init__(self, model_path):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        task = \"summarization\"\n",
        "        use_task_specific_params(self.model, task)\n",
        "        self.batch_size = 1\n",
        "        self.decoder_start_token_id = None\n",
        "\n",
        "    def generate(\n",
        "            self, \n",
        "            queries,\n",
        "            decode_method=\"beam\", \n",
        "            num_generate=5, \n",
        "            ):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            examples = queries\n",
        "\n",
        "            decs = []\n",
        "            for batch in list(chunks(examples, self.batch_size)):\n",
        "\n",
        "                batch = self.tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
        "                input_ids, attention_mask = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
        "\n",
        "                summaries = self.model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_start_token_id=self.decoder_start_token_id,\n",
        "                    num_beams=num_generate,\n",
        "                    num_return_sequences=num_generate,\n",
        "                    )\n",
        "\n",
        "                dec = self.tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "                decs.append(dec)\n",
        "\n",
        "            return decs\n",
        "\n",
        "\n",
        "all_relations = [\n",
        "    \"AtLocation\",\n",
        "    \"CapableOf\",\n",
        "    \"Causes\",\n",
        "    \"CausesDesire\",\n",
        "    \"CreatedBy\",\n",
        "    \"DefinedAs\",\n",
        "    \"DesireOf\",\n",
        "    \"Desires\",\n",
        "    \"HasA\",\n",
        "    \"HasFirstSubevent\",\n",
        "    \"HasLastSubevent\",\n",
        "    \"HasPainCharacter\",\n",
        "    \"HasPainIntensity\",\n",
        "    \"HasPrerequisite\",\n",
        "    \"HasProperty\",\n",
        "    \"HasSubEvent\",\n",
        "    \"HasSubevent\",\n",
        "    \"HinderedBy\",\n",
        "    \"InheritsFrom\",\n",
        "    \"InstanceOf\",\n",
        "    \"IsA\",\n",
        "    \"LocatedNear\",\n",
        "    \"LocationOfAction\",\n",
        "    \"MadeOf\",\n",
        "    \"MadeUpOf\",\n",
        "    \"MotivatedByGoal\",\n",
        "    \"NotCapableOf\",\n",
        "    \"NotDesires\",\n",
        "    \"NotHasA\",\n",
        "    \"NotHasProperty\",\n",
        "    \"NotIsA\",\n",
        "    \"NotMadeOf\",\n",
        "    \"ObjectUse\",\n",
        "    \"PartOf\",\n",
        "    \"ReceivesAction\",\n",
        "    \"RelatedTo\",\n",
        "    \"SymbolOf\",\n",
        "    \"UsedFor\",\n",
        "    \"isAfter\",\n",
        "    \"isBefore\",\n",
        "    \"isFilledBy\",\n",
        "    \"oEffect\",\n",
        "    \"oReact\",\n",
        "    \"oWant\",\n",
        "    \"xAttr\",\n",
        "    \"xEffect\",\n",
        "    \"xIntent\",\n",
        "    \"xNeed\",\n",
        "    \"xReact\",\n",
        "    \"xReason\",\n",
        "    \"xWant\",\n",
        "    ]\n",
        "\n",
        "print(\"model loading ...\")\n",
        "comet = Comet(\"./comet-atomic_2020_BART\")\n",
        "comet.model.zero_grad()\n",
        "print(\"model loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg8SSJmhem8Z"
      },
      "source": [
        "Run your queries. `head` is the input sentence, and `rel` is the relation, as seen in `all_relations`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UukXJgBmdskV"
      },
      "outputs": [],
      "source": [
        "# Example code to run COMET. A method has been written for you further down called callCOMET().\n",
        "queries = []\n",
        "head = \"PersonX relies on PersonY\"\n",
        "rel = \"xNeed\"\n",
        "query = \"{} {} [GEN]\".format(head, rel)\n",
        "queries.append(query)\n",
        "print(queries)\n",
        "results = comet.generate(queries, decode_method=\"beam\", num_generate=5)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-PuV-j5d8a2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Optional: Get ATOMIC data (if you wanted the train/test/val sets)\n",
        "!wget https://ai2-atomic.s3-us-west-2.amazonaws.com/data/atomic2020_data-feb2021.zip\n",
        "!unzip atomic2020_data-feb2021.zip\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJDTUKQyjHL4"
      },
      "source": [
        "# Parse the sentence to feed into COMET\n",
        "You're welcome to change this to fit your needs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4nK_609jTi_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "class SentParser:\n",
        "  \"\"\"\n",
        "  Parse the sentence and get the entities and the new phrase with tags\n",
        "  \"\"\"\n",
        "  def __init__(self, sentence):\n",
        "    sentence = sentence.replace(\".\",\"\")\n",
        "    parse = nlp(sentence) # call Stanza\n",
        "    self.phrase = sentence # original sentence\n",
        "    self.entities = dict() # dict of labels for entities e.g., [{PersonX: John}]\n",
        "    self.new_phrase = sentence # the sentence with person names changed to PersonX and PersonY\n",
        "    \n",
        "    for sentence in parse.sentences:\n",
        "      ents = self.getEntities(sentence.tokens)\n",
        "      self.entities = ents\n",
        "\n",
        "      for tag in ents.keys():\n",
        "        person = ents[tag]\n",
        "        self.new_phrase = self.new_phrase.replace(person, tag)\n",
        "        \n",
        "  def getEntities(self, parse):\n",
        "    \"\"\"\n",
        "    get the named entities so you can pass it to ATOMIC's input and\n",
        "    fill the PersonX and PersonY tags from the output\n",
        "\n",
        "    args:\n",
        "    parse (list) - list of Stanza token objects for this phrase\n",
        "\n",
        "    return:\n",
        "    entities (dict) - keeps track of who is PersonX and PersonY e.g. {PersonX: John}\n",
        "    \"\"\"\n",
        "    entities = dict()\n",
        "    count = 0\n",
        "    for word in parse:\n",
        "      if \"PERSON\" in word.ner:\n",
        "        if count == 0:\n",
        "          entities['PersonX'] = word.text \n",
        "          count+=1\n",
        "        elif count == 1:\n",
        "          entities['PersonY'] = word.text\n",
        "    return entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U_9dFk0mmcn"
      },
      "outputs": [],
      "source": [
        "# Example call\n",
        "s = SentParser(\"John went to the bank.\")\n",
        "print(s.phrase)\n",
        "print(s.entities)\n",
        "print(s.new_phrase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYd9fKe_im9A"
      },
      "source": [
        "# TODO: Setup your schema\n",
        "\n",
        "You're welcome to use all or just a subset of the relations from `all_relations`. Work with whatever you think makes sense.\n",
        "\n",
        "Tip: You might want to not take every single fact that ATOMIC gives you. Try to come up with a heuristic to just take what you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbqcULLzjC9_"
      },
      "outputs": [],
      "source": [
        "def callCOMET(sent, rel, decode=\"beam\", num=5):\n",
        "  \"\"\"\n",
        "  Making COMET generate facts based on an input sent and a relation rel\n",
        "  You can also provide the decoding method and the number of facts \n",
        "  you want it to output.\n",
        "  \"\"\"\n",
        "  query = [\"{} {} [GEN]\".format(sent, rel)]\n",
        "  gen = comet.generate(query, decode_method=decode, num_generate=num)[0]\n",
        "  return [s.strip().replace(\".\",\"\") for s in gen if s.strip() != \"none\" or s.strip() != \".\"]\n",
        "\n",
        "def fillEntityTags(fact, NER):\n",
        "  \"\"\"\n",
        "  Given a output fact from COMET (str) and the NER (dict), replace the PersonX/Y\n",
        "  tags with their original names\n",
        "  \"\"\"\n",
        "  new = fact\n",
        "  for entity in NER.keys():\n",
        "    new = new.replace(entity,NER[entity])\n",
        "  return new\n",
        "\n",
        "class Predicate:\n",
        "  \"\"\"\n",
        "  Individual precondition and effect objects\n",
        "  \"\"\"\n",
        "  def __init__(self, rel, statement, isPre, neg=False):\n",
        "    self.relation = rel # string - input COMET relation\n",
        "    self.statement = statement # string - results from COMET\n",
        "    self.isPrecondition = isPre # boolean - if it's a precondition (True) or an effect (False)\n",
        "    self.isNegated = neg # boolean - if it's negated (True) or not (False)\n",
        "\n",
        "class Schema:\n",
        "  \"\"\"\n",
        "  Your schema\n",
        "  \"\"\"\n",
        "  def __init__(self, starting_state):\n",
        "    self.state = starting_state # defaultdict(set) - a set of facts for each entity\n",
        "    self.curr_event = \"\" # string - received from the parser; phrase from the original input sentence with PersonX/PersonY labels\n",
        "    self.curr_NER = dict() # dict - person names and their corresponding tags for a given subevent/phrase\n",
        "    self.preconditions = [] # list - stores Predicate objects\n",
        "    self.timestep = 0 # int - how far you are in the story (optional)\n",
        "\n",
        "  ### Check the preconditions against the state ###\n",
        "  def checkPrecondition(self, pred):\n",
        "    \"\"\"\n",
        "    Given this precondition and the current state, does this precondition pass?\n",
        "    args:\n",
        "    pred (Predicate)\n",
        "    \n",
        "    return:\n",
        "    boolean - whether or not this event is valid\n",
        "    \"\"\"\n",
        "    if pred.isPrecondition == False: return False #it's an effect, don't consider it\n",
        "\n",
        "    ### Your code here ###\n",
        "    #TODO: check pred against self.state\n",
        "    # You can do direct matches or \"close enough\" (i.e., similarity) matches\n",
        "    #e.g.:\n",
        "    if pred.statement not in self.state[self.curr_NER['PersonX']]:\n",
        "      return False\n",
        "    ######################\n",
        "\n",
        "    return True\n",
        "\n",
        "  def getPreconditions(self):\n",
        "    \"\"\"\n",
        "    Given the input event string (self.curr_event), \n",
        "    return a list of preconditions (list of Predicate objects)\n",
        "    \"\"\"\n",
        "    pre = []  \n",
        "\n",
        "    ### Your code here ###\n",
        "    #TODO: complete the list using relations from \"all_relations\"\n",
        "    #an example:\n",
        "    rel = \"HasPrerequisite\"\n",
        "    comet_out = callCOMET(self.curr_event,rel)\n",
        "    for fact in comet_out:      \n",
        "      filled = fillEntityTags(fact,self.curr_NER)\n",
        "      pre.append(Predicate(rel, filled, True)) #Predicate(rel, statement, isPrecondition, negated)\n",
        "    ######################\n",
        "\n",
        "    return pre    \n",
        "\n",
        "  def checkValidity(self):\n",
        "    \"\"\"\n",
        "    Goes through all the preconditions to check to see if\n",
        "    this event can be added to the state.\n",
        "    A precondition is considered valid as long as \n",
        "    \n",
        "    return:\n",
        "    boolean - whether or not this event is valid\n",
        "    \"\"\"\n",
        "    print(self.state)\n",
        "    preconds = self.getPreconditions()\n",
        "    valids = []\n",
        "    for precond in preconds:\n",
        "      print(precond.statement)\n",
        "      valid = self.checkPrecondition(precond)\n",
        "      print(valid)\n",
        "      if valid:\n",
        "        valids.append(precond)\n",
        "    if valids:\n",
        "      self.preconditions += valids\n",
        "      return True    \n",
        "    return False\n",
        "\n",
        "\n",
        "  ### Once validated, update the schema ###\n",
        "  def getEffects(self):\n",
        "    \"\"\"\n",
        "    Given the input event string (self.curr_event),\n",
        "    return a list of effects (list of Predicate objects)\n",
        "    \"\"\"\n",
        "    effects = []\n",
        "\n",
        "    ### Your code here ###\n",
        "    #TODO: complete the list using relations from \"all_relations\"\n",
        "    #an example:\n",
        "    rel = \"xReact\"\n",
        "    fact = callCOMET(self.curr_event,rel)[0]\n",
        "    filled = fillEntityTags(fact,self.curr_NER)\n",
        "    effects.append(Predicate(rel, filled, False)) #Predicate(rel, statement, isPrecondition, negated)\n",
        "    ######################\n",
        "\n",
        "    return effects   \n",
        "\n",
        "\n",
        "  def updateSchema(self, event, NER_dict):\n",
        "    \"\"\"\n",
        "    Given an input event string (event), check the validity of adding it to the state,\n",
        "    and update the schema state (self.state) with new effects\n",
        "    args:\n",
        "    event(str) - received from the parser; phrase from the original input sentence with PersonX/PersonY labels\n",
        "    NER_dict(dict) - person names and their corresponding tags for a given subevent/phrase, e.g. NER_dict['PersonX'] = \"Cindy\"\n",
        "    \"\"\"\n",
        "    self.curr_event = event\n",
        "    self.curr_NER = NER_dict\n",
        "\n",
        "    #1) check validity\n",
        "    valid = self.checkValidity()\n",
        "    print(\"Preconditions\",self.preconditions[0].statement)\n",
        "\n",
        "    #2) update the state\n",
        "    effects = self.getEffects()\n",
        "    print(\"Effects\", effects[0].statement)\n",
        "\n",
        "\n",
        "    ### Your code here ###\n",
        "\n",
        "    #TODO: add these new effects to the state    \n",
        "    for effect in effects:\n",
        "      if effect.relation == \"xReact\":\n",
        "        self.state[NER_dict['PersonX']].add(effect.statement)\n",
        "\n",
        "    # TODO: (optional) remove facts that aren't true anymore\n",
        "    # Although this is ideal, it might be hard to figure out when facts are negated\n",
        "\n",
        "    ######################\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEbMOVcneM0X"
      },
      "source": [
        "# What to Turn In\n",
        "* Your code\n",
        "* A brief description/diagram of the structure of your schema\n",
        "* Your answers to these two sets of questions:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V31TrjW2UDJW"
      },
      "source": [
        "## Story Tracking Questions\n",
        "Run the following stories through your system and print out your schema after each sentence (or subevent if there are multiple events in a sentence).\n",
        "\n",
        "For each scenario, report:\n",
        "- the number of the scenario,\n",
        "- the bit of code that runs the story, and\n",
        "- keep the output of your schema on that story in your ipynb. It should be printing the schema at each timestep. (No fancy formatting is necessary. Just printing the dictionary is fine.)\n",
        "\n",
        "Note: The first sentence of each story should initialize your schema state.\n",
        "\n",
        "**Do not change your schema code in between running these examples! Your final schema code should be able to run multiple scenarios.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjIg_wvKMNjQ"
      },
      "outputs": [],
      "source": [
        "stories = {\n",
        "    1: [\"Gina misplaced her phone.\", \"Gina looks for her phone in the living room.\", \"Gina remembers leaving her phone in the car.\", \"Gina goes back to the car.\", \"Gina finds her phone in the car.\"],\n",
        "    2: [\"Phil was at the community pool.\",\"Phil thought he could go out to the deeper end by himself.\",\"Phil jumps into the deep end.\",\"Phil has trouble staying afloat.\",\"The lifeguard had to help Phil out of the water.\"],\n",
        "    3: [\"Amy was happy her first class in junior high was all new kids.\", \"Amy introduced herself to the girl seated next to her.\", \"The girl was even more nervous than Amy to make friends.\", \"The girls talked and bonded over their love of books.\", \"The girls decided to meet up after school to go to the library.\"],\n",
        "    4: [\"Xander's dog hates his treats.\", \"Xander decided to go buy some new dog treats.\", \"None of the dog treats at the pet store looked tasty.\", \"Xander decided to buy his dog some salmon from the fish market.\", \"Xander's dog loved the salmon.\"],\n",
        "    5: [\"Tim has never cooked for his family.\", \"Tim decided to follow an old family recipe.\", \"Tim's grandma told him anybody could make the recipe.\", \"Tim made a whole meal for his family in one hour.\", \"Tim's family all loved the meal.\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkN6CxQ8PNbq"
      },
      "outputs": [],
      "source": [
        "# This is where everything is called\n",
        "start = defaultdict(set)\n",
        "start.update({\"John\": set([\"John is hungry\"])})\n",
        "\n",
        "# Example call\n",
        "schema = Schema(start)\n",
        "sent = \"John eats an apple.\"\n",
        "s = SentParser(sent)\n",
        "schema.updateSchema(s.new_phrase,s.entities)\n",
        "print(schema.state)\n",
        "\n",
        "\n",
        "# Uncomment this when you're ready\n",
        "\"\"\"\n",
        "\n",
        "# Running through the above stories\n",
        "for i in range(1,7):\n",
        "  start = defaultdict(set)\n",
        "\n",
        "  ### Your code here ###\n",
        "  # TODO: initialize your start state with the first sentence of the story\n",
        "  # (You can do this here or inside of the Schema object)\n",
        "  start['John'] = set([\"John is hungry\"])\n",
        "  ######################\n",
        "\n",
        "  schema = Schema(start)\n",
        "  for sent in stories[i]:\n",
        "    s = SentParser(sent)\n",
        "    schema.updateSchema(s.new_phrase,s.entities)\n",
        "    print(schema.state)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TS3kuHsUQck"
      },
      "source": [
        "## Questions about COMET-ATOMIC\n",
        "\n",
        "\n",
        "\n",
        "1.   What types of stories is COMET-ATOMIC *good* at tracking? In other words, what types of information is it good at modeling? (It might help to think about how COMET-ATOMIC compares to other knowledge bases.)\n",
        "2.   What types of stories is COMET-ATOMIC *bad* at tracking?\n",
        "\n",
        "   2a. Do you think any of the other knowledge bases mentioned in class could better model these?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egAZqOEKxS4P"
      },
      "source": [
        "# Grading Rubric\n",
        "This assignment is 10% of your final grade, broken down as the following for grading:\n",
        "*   Code - 2 points\n",
        "*   Schema Diagram/Explanation - 2 points\n",
        "*   Story Tracking Questions - 15 points\n",
        "*   COMET-ATOMIC Questions - 6 points\n",
        "\n",
        "Total: 20 points\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW5-Schemas.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}