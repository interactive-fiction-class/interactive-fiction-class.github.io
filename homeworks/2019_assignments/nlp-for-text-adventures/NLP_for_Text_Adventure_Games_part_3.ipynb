{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP for Text Adventure Games - part 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH0POdh6BUKz",
        "colab_type": "text"
      },
      "source": [
        "# NLP for Text Adventure Games - part 3\n",
        "\n",
        "This notebook contains a variety of other NLP technologies that you might find useful for creating intereting text adventure games.  \n",
        "\n",
        "There are no tasks for you to do in this notebook, and nothing that you have to submit with your homework.  The purpose is just give some ideas of things that you might be able to incorporate into your game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ktwkZqsbsO5"
      },
      "source": [
        "# Dependency Parsing\n",
        "\n",
        "A dependency parser creates determines how each word relates to its parent.  It creates a graph, where each word has exactly one parent.  Each edge is labeled with a grammatical role, like direct object (dobj) or prepositional object (pobj).  \n",
        "\n",
        "Another related technology is [Semantic Role Labeling](https://demo.allennlp.org/semantic-role-labeling), which extracts the main verb and its objects.\n",
        "\n",
        "Here's a demo of the [AllenNLP dependency parser](https://demo.allennlp.org/dependency-parsing)\n",
        "\n",
        "<div>\n",
        "<a href=\"https://demo.allennlp.org/dependency-parsing\"><img src=\"https://github.com/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/nlp-for-text-adventures/parse.png?raw=true\" width=\"500\"/></a>\n",
        "</div>\n",
        "\n",
        "**Game Idea:** Instead of hardcoding \"verb object\" into your parser, use the dependency parse of the command to automaticly extract the verbs and corresponding direct objects from the player's command. This should allow you to support strings of commands.\n",
        "\n",
        "An example of parsing the verb and direct object of a command is shown below.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXjlHeSLN_0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBiSjYUEPBDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_hlGl46PRlG",
        "colab_type": "code",
        "outputId": "3948aa70-20e6-4898-8198-71ddaa2ec0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "def verb_object_pairs(sentence):\n",
        "  print('Sentence: ')\n",
        "  print(sentence)\n",
        "\n",
        "  prediction = predictor.predict(sentence=sentence)\n",
        "\n",
        "  words = prediction['words']\n",
        "  pred_dependencies = prediction['predicted_dependencies']\n",
        "  pred_heads = prediction['predicted_heads']\n",
        "\n",
        "  pairs = []\n",
        "  for i in range(len(words)):\n",
        "    if pred_dependencies[i] == 'dobj':\n",
        "      verb =  words[pred_heads[i]-1] # -1 is bc head indices are one-indexed\n",
        "      direct_object = words[i]\n",
        "      pairs.append((verb, direct_object))\n",
        "  return pairs\n",
        "\n",
        "print(verb_object_pairs(\"Take the apple from the table and eat it.\"))\n",
        "print(verb_object_pairs(\"Taunt the dragon before slaying him with my sword.\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \n",
            "Take the apple from the table and eat it.\n",
            "[('Take', 'apple'), ('eat', 'it')]\n",
            "Sentence: \n",
            "Taunt the dragon before slaying him with my sword.\n",
            "[('Taunt', 'dragon'), ('slaying', 'him')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFYRQgrVX1H8",
        "colab_type": "text"
      },
      "source": [
        "# Coreference resolution\n",
        "\n",
        "You may have noticed in the previous section that we end up with verb-object pairs where the object is a pronoun.\n",
        "\n",
        "Pronouns are words that refer to an entity that has already been mentioned in the text or is a participant in the conversation.\n",
        "\n",
        "In English, pronouns are:\n",
        "\n",
        "<div>\n",
        "<img src=\"https://live.staticflickr.com/626/31598952693_017b53571c_c.jpg\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Since the commands in your text-adventure game are all in [inperative form](https://grammar.collinsdictionary.com/easy-learning/the-imperative), you will really only need to deal with pronouns being used as direct objects (the left column above).\n",
        "\n",
        "You can use a coreference resolution algorithm to resolve the \"it\" in `Take the apple from the table and eat it.` or the \"him\" in `\"Taunt the dragon before slaying him with my sword.`.\n",
        "\n",
        "## Challenges with Coreference Resolution\n",
        "Play around with AllenNLP's coreference resolution demo [here](https://demo.allennlp.org/coreference-resolution).\n",
        "\n",
        "You'll notice that the system is far from perfect. AllenNLP predicts that the \"it\" is actually the table. This is a result of the inherent ambiguity in English language. There are a couple ways you can try to deal with this in your game.\n",
        "\n",
        "1. Use auxiliary linguistic information (word embeddings perhaps) to figure out which entity is more likely being referenced.\n",
        "2. Incorporate the coreference resolution algorithm's likely mistakes into the gameplay experience, adding humor. For example:\n",
        "\n",
        "```\n",
        "THE ROOM CONTAINS A SINGLE WOODEN TABLE. THERE IS A SHINY RED APPLE SITTING ON IT.\n",
        "> Take the apple from the table and eat it.\n",
        "YOU PUT THE APPLE INTO YOUR INVENTORY. YOU ATTEMPT TO TAKE A BITE OUT OF THE TABLE...OUCH! THAT HURT YOUR TEETH!\n",
        "> Eat the apple.\n",
        "THE APPLE TASTES DELICIOUS. HOWEVER, YOU SUDDENLY START TO FEEL VERY SLEEPY.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnvR-oBxfLrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KisDnnLqfREL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV9ArumXfbQB",
        "colab_type": "code",
        "outputId": "2d67c111-836e-4067-f223-f1d6110aa1b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "def coreference_resolution(text):\n",
        "  print(text)\n",
        "  prediction = predictor.predict(document=text)\n",
        "  print(prediction)\n",
        "  clusters = prediction['clusters']\n",
        "  words = prediction['document']\n",
        "  for cluster in clusters:\n",
        "    entity_indices, pronoun_indices = cluster\n",
        "\n",
        "    entity_str = words[entity_indices[0]:entity_indices[1]+1]\n",
        "    pronoun_str = words[pronoun_indices[0]:pronoun_indices[1]+1]\n",
        "    print('\"%s\" references \"%s\"' % (pronoun_str, entity_str))\n",
        "\n",
        "coreference_resolution(\"Take the apple from the table and eat it.\")\n",
        "coreference_resolution(\"John takes the apple from the table, and he eats it.\")\n",
        "coreference_resolution(\"Take the apple from the table and eat it. John likes to eat apples.\")\n",
        "coreference_resolution(\"Taunt the dragon before slaying him.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taunt the dragon before slaying him.\n",
            "{'top_spans': [[1, 2], [5, 5]], 'predicted_antecedents': [-1, -1], 'document': ['Taunt', 'the', 'dragon', 'before', 'slaying', 'him', '.'], 'clusters': []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ0_xsrK_w0R",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis\n",
        "A common classification problem is detecting whether a text has positive or negartive sentiment. \n",
        "\n",
        "A library called [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis) provides a pre-trained sentiment model that you can use.\n",
        "\n",
        "**Game Idea:**\n",
        "There are two guards to get past, one that only lets you through if you insult him, another only if you complement him."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkaXhTygHNyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "\n",
        "text = '''\n",
        "I enjoyed my stay tremendously; what incredible service!\n",
        "The castle was absolutely incredible to visit, especially its voluminous dungeons.\n",
        "My expectations were high, but the castle ended up being only so-so.\n",
        "You're a despicable excuse for a guard; it's a wonder you were hired.\n",
        "'''\n",
        "\n",
        "blob = TextBlob(text)\n",
        "\n",
        "for sentence in blob.sentences:\n",
        "    print(sentence.sentiment.polarity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFOcBkRs6UE2",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Word Concreteness\n",
        "\n",
        "Concreteness is a measure of how readily the concerpt repreesented by a word can be seen, smelled, heard, or felt. \n",
        "\n",
        "If a concept can be readily perceived by the senses then is is very concrete. If a concept cannot be perceived, then it is the opposite of concrete--abstract.\n",
        "\n",
        "It's possible from a word's embedding to prdict how concrete the word is.  In fact, Daphne did this in [a really cool publication that has pictures of cute kittens in it](https://www.cis.upenn.edu/~ccb/publications/learning-translations-via-images.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8OYKpet7xNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -N http://crr.ugent.be/papers/Concreteness_ratings_Brysbaert_et_al_BRM.txt\n",
        "\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from zlib import crc32\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import scipy.stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yf55plCJ8I",
        "colab_type": "text"
      },
      "source": [
        "#### Helper methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2_Cy9z56Sp2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def read_in_data(file_path, word2vec):\n",
        "  words = []\n",
        "  concs = []\n",
        "  embs = []\n",
        "  with open(file_path) as csvfile:\n",
        "    reader = csv.DictReader(csvfile, delimiter='\\t',)\n",
        "    for row in tqdm(reader):\n",
        "      conc = float(row['Conc.M'])\n",
        "      word = row['Word']\n",
        "      if conc != 0:\n",
        "        # 0 means there was not enough interannotator agreement for them to\n",
        "        # include the score.\n",
        "\n",
        "        word = word.replace(' ', '-').lower()\n",
        "        if word in word2vec:\n",
        "          # For now, skip words not in the embedding file. \n",
        "          embs.append(word2vec.query(word))\n",
        "          words.append(word)\n",
        "          concs.append(conc)\n",
        "  return words, concs, embs\n",
        "\n",
        "def floathash(b):\n",
        "  return float(crc32(b.encode('utf-8')) & 0xffffffff) / 2**32\n",
        "\n",
        "def create_split(words, concs, embs, train_prob = 0.9):\n",
        "  val_words = []\n",
        "  val_concs = []\n",
        "  val_embs = []\n",
        "\n",
        "  train_words = []\n",
        "  train_concs = []\n",
        "  train_embs = []\n",
        "\n",
        "  for word, conc, emb in zip(words, concs, embs):\n",
        "    if floathash(word) <= train_prob:\n",
        "      train_words.append(word)\n",
        "      train_concs.append(conc)\n",
        "      train_embs.append(emb)\n",
        "    else:\n",
        "      val_words.append(word)\n",
        "      val_concs.append(conc)\n",
        "      val_embs.append(emb)\n",
        "  return train_words, train_concs, train_embs, val_words, val_concs, val_embs \n",
        "\n",
        "def crush_scores(scores):\n",
        "  \"\"\"Turn 1-5 scores to 0-1 scale.\"\"\"\n",
        "  return [(s - 1) / 4.0 for s in scores]\n",
        "\n",
        "def train_model(train_embs, train_concs, val_embs, val_concs, method='linear', normalize=False):\n",
        "  print('Training with method %s, %s' % (method, '[0,1]' if normalize else '[1,4]'))\n",
        "  if normalize:\n",
        "    val_concs = crush_scores(val_concs)\n",
        "    train_concs = crush_scores(train_concs)    \n",
        "  if method == 'linear':\n",
        "    model = LinearRegression()\n",
        "  elif method == '2mlp':\n",
        "    model = MLPRegressor(hidden_layer_sizes=[64,32])\n",
        "  else:\n",
        "    raise ValueError('Unsupported method')\n",
        "\n",
        "  model = model.fit(train_embs, train_concs)\n",
        "  print('Train correlation: ')\n",
        "  print(scipy.stats.pearsonr(model.predict(train_embs), train_concs))\n",
        "  \n",
        "  print('Val correlation: ')\n",
        "  print(scipy.stats.pearsonr(model.predict(val_embs), val_concs))\n",
        "  \n",
        "  print('')\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng_-kzLnCH6p",
        "colab_type": "text"
      },
      "source": [
        "#### Read in data and train small model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZppknFY6dg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words, concs, embs = read_in_data('Concreteness_ratings_Brysbaert_et_al_BRM.txt', word2vec)\n",
        "\n",
        "train_words, train_concs, train_embs, val_words, val_concs, val_embs = create_split(words, concs, embs, 0.95)\n",
        "print('Train set size: %d' % len(train_words))\n",
        "print('Val set size: %d' % len(val_words))\n",
        "\n",
        "model = train_model(train_embs, train_concs, val_embs, val_concs, '2mlp', True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5toECmICEPd",
        "colab_type": "text"
      },
      "source": [
        "### Some predictions for words not in train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36pt5I5PB3xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('archetype' in train_words)\n",
        "print(model.predict([word2vec.query('archetype')]))\n",
        "\n",
        "print('pigtailed' in train_words)\n",
        "print(model.predict([word2vec.query('pigtailed')]))\n",
        "\n",
        "print('determination' in train_words)\n",
        "print(model.predict([word2vec.query('determination')]))\n",
        "\n",
        "print('whirlpool' in train_words)\n",
        "print(model.predict([word2vec.query('whirlpool')]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3uLr-zoAE_R",
        "colab_type": "text"
      },
      "source": [
        "# BERT Contextual Word Embeddings\n",
        "One issue with word embeddings is that they don't handle ambiguity. If I say the word \"bat\", do you picture baseball or a cute flying mammal?  Word2vec would end up picking a vector somewhere in between the two.\n",
        "\n",
        "Contextual word embeddings are word embeddings that vary based on the context in which a word is being used.\n",
        "\n",
        "Consider the following sentences.\n",
        "```\n",
        "1) The bat comes out at night to eat mosquitoes.\n",
        "2) The swallow flitted from branch to branch, eating mosquitoes.\n",
        "3) The player dropped the bat and sprinted past first base.\n",
        "```\n",
        "\n",
        "With contextual word embeddings, the embedding of \"bat\" in (1) will end up being close to the embedding for \"swallow\" in  (2) than the embedding of \"bat\" in (3).\n",
        "\n",
        "BERT is a neural network trained to produce one embedding per token in the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH15OHJuDzX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(val_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhDsiBBmcR3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCz6l67sCuah",
        "colab_type": "text"
      },
      "source": [
        "#### Helper methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym67qJ8IWsmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens_and_embeddings(text):\n",
        "  inputs_ids = tokenizer.encode(text)\n",
        "  input_ids = torch.tensor(inputs_ids).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "  token_embeddings, merged_embedding = model(input_ids)\n",
        "\n",
        "  # Remove the embeddings in the first and last positions\n",
        "  # which are the [CLS] and [SEP] tokens.\n",
        "  token_embeddings = token_embeddings.squeeze()[1:-1, :]\n",
        "  return token_embeddings.detach().numpy()\n",
        "\n",
        "def token_indexes_for_word(tokens, word):\n",
        "  \"\"\"Returns the token indexes corresponding to the specified word.\"\"\"\n",
        "  ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  word_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
        "  word_len = len(word_ids)\n",
        "\n",
        "  for i in range(len(tokens) - word_len):\n",
        "    if np.all(np.equal(ids[i:(i+word_len)], word_ids)):\n",
        "      return list(range(i, i+word_len))\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePqEXHw2C392",
        "colab_type": "text"
      },
      "source": [
        "#### Computing a word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOOhNoh2adZ1",
        "colab_type": "code",
        "outputId": "25b188c0-1d62-4a60-c65e-16dcc40d8954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Since BERT uses a subword vocabulary can take up multiple tokens.\n",
        "# This can be seen in the word \"mosquitoes\" in the following sentence.\n",
        "sentence = \"The bat comes out at night to eat mosquitoes.\"\n",
        "embeddings = get_tokens_and_embeddings(sentence)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "mosquitoes_indices = token_indexes_for_word(tokens, \"mosquitoes\")\n",
        "print(sentence)\n",
        "print(tokens)\n",
        "print(\"'mosquitoes' is in token positions: %s\" % str(mosquitoes_indices))\n",
        "\n",
        "# For 'mosquitoes' and other multi-token words, a single embedding for the word\n",
        "# can be computed by simply taking the embedding of the first token of the word.\n",
        "# Another option is to take the mean over all of the constituent token\n",
        "# embeddings.\n",
        "mosquitoes_embedding = embeddings[mosquitoes_indices[0], :]\n",
        "alternative_mosquitoes_embedding = np.mean(embeddings[mosquitoes_indices, :], axis=0)\n",
        "print(mosquitoes_embedding.shape)\n",
        "print(alternative_mosquitoes_embedding.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The bat comes out at night to eat mosquitoes.\n",
            "['the', 'bat', 'comes', 'out', 'at', 'night', 'to', 'eat', 'mosquito', '##es', '.']\n",
            "'mosquitoes' is in token positions: [8, 9]\n",
            "(768,)\n",
            "(768,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcROMxH0F4zb",
        "colab_type": "text"
      },
      "source": [
        "#### Comparing contextual word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnpDKEG9XAQV",
        "colab_type": "code",
        "outputId": "103a5481-ba38-4268-bd3e-4f97397a5a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "sentence = \"The bat comes out at night to eat mosquitoes.\"\n",
        "embeddings = get_tokens_and_embeddings(sentence)\n",
        "animalbat_index = token_indexes_for_word(tokens, \"bat\")[0]\n",
        "animalbat_embedding = embeddings[animalbat_index, :]\n",
        "\n",
        "sentence = \"The swallow flitted from branch to branch, eating mosquitoes.\"\n",
        "embeddings = get_tokens_and_embeddings(sentence)\n",
        "swallow_index = token_indexes_for_word(tokens, \"bat\")[0]\n",
        "swallow_embedding = embeddings[swallow_index, :]\n",
        "\n",
        "sentence = \"The player dropped the bat and sprinted past first base.\"\n",
        "embeddings = get_tokens_and_embeddings(sentence)\n",
        "baseballbat_index = token_indexes_for_word(tokens, \"bat\")[0]\n",
        "baseballbat_embedding = embeddings[baseballbat_index, :]\n",
        "\n",
        "print('Distance between a swallow and an animal bat: %f' %\n",
        "      cosine(animalbat_embedding, swallow_embedding))\n",
        "print('Distance between an animal bat and a baseball bat: %f' %\n",
        "      cosine(animalbat_embedding, baseballbat_embedding))\n",
        "print('Distance between a swallow and a baseball bat: %f' %\n",
        "      cosine(swallow_embedding, baseballbat_embedding))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distance between a swallow and an animal bat: 0.346092\n",
            "Distance between an animal bat and a baseball bat: 0.666941\n",
            "Distance between a swallow and a baseball bat: 0.706544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoFla9UYFNNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}